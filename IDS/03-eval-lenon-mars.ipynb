{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:34:01.372127Z",
     "start_time": "2019-01-16T10:33:59.687460Z"
    },
    "code_folding": [
     21,
     82
    ]
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pyearth import Earth\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import Bunch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.linear_model import Ridge\n",
    "import random\n",
    "from scipy.special import gammainccinv, erfinv\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import chi2, norm\n",
    "from numpy.random import default_rng\n",
    "from types import LambdaType\n",
    "\n",
    "\n",
    "class Dataset(Enum):\n",
    "  GENERATED = 'Generated'\n",
    "  BLOBS = 'Blobs'\n",
    "  IRIS2D = 'Iris (2D)'\n",
    "  IRIS = 'Iris'\n",
    "  BC = 'Breast cancer'\n",
    "  PIMA = 'Pima Indian'\n",
    "  WINE = 'Wine'\n",
    "\n",
    "  @property\n",
    "  def instance(self):\n",
    "    if self == Dataset.GENERATED:\n",
    "      amount = 2500\n",
    "      data = np.random.uniform(0,[3,10],(amount,2))\n",
    "      return Bunch(\n",
    "        data=data,\n",
    "        feature_names=['feature 1', 'feature 2'],\n",
    "        target_names=['yellow', 'blue'],\n",
    "        target=(np.logical_and(data[:,0]>1, data[:,1]>2)).astype(int)\n",
    "      )\n",
    "\n",
    "    if self == Dataset.BLOBS:\n",
    "      from sklearn.datasets import make_blobs\n",
    "      amount = 250\n",
    "      X,y = make_blobs(amount, n_features = 3, centers = 4)\n",
    "      return Bunch(\n",
    "        data=X,\n",
    "        feature_names=['feature 1', 'feature 2', 'feature 3'],\n",
    "        target_names=['yellow', 'blue'],\n",
    "        target=(y>1).astype(int)\n",
    "      )\n",
    "\n",
    "    if self == Dataset.IRIS2D:\n",
    "      from sklearn.datasets import load_iris\n",
    "      dataset = load_iris()\n",
    "      indexes = np.array([0,1])\n",
    "      dataset.data = dataset.data[:, indexes]\n",
    "      dataset.feature_names = np.array(dataset.feature_names)[indexes]\n",
    "      return dataset\n",
    "\n",
    "    if self == Dataset.IRIS:\n",
    "      from sklearn.datasets import load_iris\n",
    "      return load_iris()\n",
    "\n",
    "    if self == Dataset.BC:\n",
    "      from sklearn.datasets import load_breast_cancer\n",
    "      dataset = load_breast_cancer()\n",
    "      return dataset\n",
    "\n",
    "    if self == Dataset.WINE:\n",
    "      from sklearn.datasets import load_wine\n",
    "      dataset = load_wine()\n",
    "      return dataset\n",
    "\n",
    "    if self == Dataset.PIMA:\n",
    "      from sklearn.datasets import load_diabetes\n",
    "      dataset = load_diabetes()\n",
    "      dataset.target = (dataset.target > 200).astype(int)\n",
    "      return dataset\n",
    "\n",
    "\n",
    "class Classifier(Enum):\n",
    "  DT = 'Decision tree'\n",
    "  RF = 'Random Forest'\n",
    "  MLP = 'Perceptron'\n",
    "  GNB = 'Naive Bayes'\n",
    "  SVM = 'Support vector machine'\n",
    "  QDA = 'Quadratic Discriminant Analysis'\n",
    "\n",
    "  @property\n",
    "  def instance(self):\n",
    "    if self == Classifier.DT:\n",
    "      from sklearn.tree import DecisionTreeClassifier\n",
    "      return DecisionTreeClassifier(random_state=1)\n",
    "    if self == Classifier.RF:\n",
    "      from sklearn.ensemble import RandomForestClassifier\n",
    "      return RandomForestClassifier(random_state=1, n_estimators=200)\n",
    "    elif self == Classifier.MLP:\n",
    "      from sklearn.neural_network import MLPClassifier\n",
    "      return MLPClassifier(alpha=0.1, hidden_layer_sizes=(100,100,100))\n",
    "    elif self == Classifier.GNB:\n",
    "      from sklearn.naive_bayes import GaussianNB\n",
    "      return GaussianNB()\n",
    "    elif self == Classifier.SVM:\n",
    "      from sklearn.svm import SVC\n",
    "      return SVC(probability=True)\n",
    "    elif self == Classifier.QDA:\n",
    "      from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "      return QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T11:06:12.727852Z",
     "start_time": "2019-01-16T11:06:12.702853Z"
    },
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from bisect import bisect\n",
    "from scipy.optimize import newton\n",
    "from functools import partial\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "class LemonExplainer(object):\n",
    "  \"\"\"\n",
    "  Intantiates the explainer.\n",
    "  \"\"\"\n",
    "  def __init__(self, training_data, distance_kernel=None, sample_size = 5000, radius_max=1, random_state=None):\n",
    "    self.random_state = check_random_state(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    self.training_data = training_data\n",
    "    self.scaler = StandardScaler(with_mean=False)\n",
    "    self.scaler.fit(training_data)\n",
    "\n",
    "    # Create hypersphere samples. The sphere is only computed once for performance and stability,\n",
    "    # but it would be better to resample the sphere every time `explain_instance` is called.\n",
    "    # I checked, this does not affect the results in any way.\n",
    "    dimensions = training_data.shape[1]\n",
    "\n",
    "    if distance_kernel is None:\n",
    "      self.distance_kernel = np.vectorize(lambda x: x ** (1 / dimensions))\n",
    "    else:\n",
    "      self.distance_kernel = np.vectorize(self._transform(distance_kernel, dimensions, radius_max=radius_max))\n",
    "\n",
    "    sphere = np.random.normal(size=(sample_size, dimensions))\n",
    "    sphere = normalize(sphere)\n",
    "    sphere *= self.distance_kernel(np.random.uniform(size=sample_size)).reshape(-1,1)\n",
    "\n",
    "    self.sphere = sphere\n",
    "\n",
    "  @property\n",
    "  def surrogate(self):\n",
    "    try:\n",
    "      return self._surrgate\n",
    "    except AttributeError:\n",
    "      self._surrogate = Ridge(alpha=0, fit_intercept=True, normalize=True, random_state=self.random_state)\n",
    "      return self._surrogate\n",
    "\n",
    "  def explain_instance(self, instance, predict_fn, labels=(1,), surrogate=None):\n",
    "    surrogate = surrogate or self.surrogate\n",
    "\n",
    "    # Create transfer dataset by perturbing the original instance with the hypersphere samples\n",
    "    X_transfer = self.scaler.inverse_transform(self.sphere) + np.array([instance])\n",
    "    y_transfer = predict_fn(X_transfer)\n",
    "\n",
    "    def explain_label(label):\n",
    "      surrogate.fit(X_transfer, y_transfer[:,label])\n",
    "      score = surrogate.score(X_transfer, y_transfer[:,label])\n",
    "      return (surrogate.coef_, score)\n",
    "\n",
    "    return [explain_label(label) for label in labels]\n",
    "\n",
    "  def _transform(self, kernel, dimensions, sample_size = 1000, radius_max=1):\n",
    "    \"\"\"\n",
    "    Inverse transform sampling\n",
    "    \"\"\"\n",
    "    cdf_samples = np.array([kernel(x)*(x**(dimensions-1)) for x in np.linspace(0, radius_max, sample_size)])\n",
    "    cdf_samples = np.cumsum(cdf_samples)\n",
    "    cdf_samples /= cdf_samples[-1]\n",
    "    return lambda y: radius_max * (bisect(cdf_samples, y) / sample_size)\n",
    "\n",
    "\n",
    "def uniform_kernel(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def gaussian_kernel(x, kernel_width):\n",
    "    # https://github.com/marcotcr/lime/blob/fd7eb2e6f760619c29fca0187c07b82157601b32/lime/lime_tabular.py#L251\n",
    "    return np.sqrt(np.exp(-(x ** 2) / kernel_width ** 2))\n",
    "\n",
    "\n",
    "def sqcos_kernel(x):\n",
    "    return np.cos(x)**2\n",
    "\n",
    "\n",
    "def trapezoid_kernel(x, a, b):\n",
    "    if 0 <= x and  x <= a:\n",
    "        return (2 / (a + b))\n",
    "    elif a <= x and x <= b:\n",
    "        return (2 / (a + b)) * ((b - x) / (b - a))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# For the `plot_sample_data` debug feature in the next cell, you need to patch LIME as such:\n",
    "\n",
    "# Change return statement from `explain_instance_with_data` (lime_base.py:204) to\n",
    "\n",
    "#         return (easy_model.intercept_,\n",
    "#                 sorted(zip(used_features, easy_model.coef_),\n",
    "#                        key=lambda x: np.abs(x[1]), reverse=True),\n",
    "#                 prediction_score, local_pred, neighborhood_data[:, used_features], weights)\n",
    "\n",
    "# And `ret_exp` in `explain_instance` (lime_tabular.py:450) to\n",
    "\n",
    "#             (ret_exp.intercept[label],\n",
    "#              ret_exp.local_exp[label],\n",
    "#              ret_exp.score, ret_exp.local_pred, ret_exp.data, ret_exp.weights) = self.base.explain_instance_with_data(\n",
    "#                     scaled_data,\n",
    "#                     yss,\n",
    "#                     distances,\n",
    "#                     label,\n",
    "#                     num_features,\n",
    "#                     model_regressor=model_regressor,\n",
    "#                     feature_selection=self.feature_selection)\n",
    "\n",
    "#print sklearn.__version__\n",
    "import sklearn\n",
    "\n",
    "\n",
    "print(\"sklearn version:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T11:06:20.844684Z",
     "start_time": "2019-01-16T11:06:13.964182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 153\u001b[0m\n\u001b[0;32m    151\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(Xtr, data\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(Xtr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 153\u001b[0m points, points2 \u001b[38;5;241m=\u001b[39m \u001b[43mdo_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_sample_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m score_pwla \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(points[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    155\u001b[0m score_lime \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(points2[:,\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 94\u001b[0m, in \u001b[0;36mdo_experiment\u001b[1;34m(Xtr, classifier, KERNEL_SIZE, plot_sample_data)\u001b[0m\n\u001b[0;32m     80\u001b[0m explanation_lime \u001b[38;5;241m=\u001b[39m explainer_lime\u001b[38;5;241m.\u001b[39mexplain_instance(\n\u001b[0;32m     81\u001b[0m     testInstance[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     82\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mpredict_proba,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     model_regressor\u001b[38;5;241m=\u001b[39msurrogate_lime\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m#     # Generate explanation\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# pwla_explanation = pwla_explainer.explain_instance(\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#     testInstance[0], classifier.predict_proba, num_samples=sample_size_lime, labels=(testInstance[1],),model_regressor=surrogate_pwla\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m explanation_lemon \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer_lemon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestInstance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestInstance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43msurrogate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msurrogate_lemon\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m## Misleading: self-reported scores from LIME and LEMON\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# lime_fidelity = explanation_lime.score\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# lemon_fidelity = explanation_lemon[0][1]\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m## Generate 50000 new samples in the neighborhood to compute R^2 between reference and surrogate models.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36mLemonExplainer.explain_instance\u001b[1;34m(self, instance, predict_fn, labels, surrogate)\u001b[0m\n\u001b[0;32m     55\u001b[0m   score \u001b[38;5;241m=\u001b[39m surrogate\u001b[38;5;241m.\u001b[39mscore(X_transfer, y_transfer[:,label])\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (surrogate\u001b[38;5;241m.\u001b[39mcoef_, score)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [explain_label(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     55\u001b[0m   score \u001b[38;5;241m=\u001b[39m surrogate\u001b[38;5;241m.\u001b[39mscore(X_transfer, y_transfer[:,label])\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (surrogate\u001b[38;5;241m.\u001b[39mcoef_, score)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mexplain_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "Cell \u001b[1;32mIn[2], line 54\u001b[0m, in \u001b[0;36mLemonExplainer.explain_instance.<locals>.explain_label\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexplain_label\u001b[39m(label):\n\u001b[1;32m---> 54\u001b[0m   \u001b[43msurrogate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_transfer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_transfer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m   score \u001b[38;5;241m=\u001b[39m surrogate\u001b[38;5;241m.\u001b[39mscore(X_transfer, y_transfer[:,label])\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (surrogate\u001b[38;5;241m.\u001b[39mcoef_, score)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\XAI_IDS\\venv\\lib\\site-packages\\sklearn_contrib_py_earth-0+untagged.22.gc4dacee.dirty-py3.9-win-amd64.egg\\pyearth\\earth.py:618\u001b[0m, in \u001b[0;36mEarth.fit\u001b[1;34m(self, X, y, sample_weight, output_weight, missing, xlabels, linvars)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_pass(X, y,\n\u001b[0;32m    615\u001b[0m                   sample_weight, output_weight, missing,\n\u001b[0;32m    616\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxlabels_, linvars, skip_scrub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_pruning \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpruning_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mskip_scrub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmooth\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth:\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_\u001b[38;5;241m.\u001b[39msmooth(X)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\XAI_IDS\\venv\\lib\\site-packages\\sklearn_contrib_py_earth-0+untagged.22.gc4dacee.dirty-py3.9-win-amd64.egg\\pyearth\\earth.py:802\u001b[0m, in \u001b[0;36mEarth.pruning_pass\u001b[1;34m(self, X, y, sample_weight, output_weight, missing, skip_scrub)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;66;03m# Do the actual work\u001b[39;00m\n\u001b[0;32m    799\u001b[0m pruning_passer \u001b[38;5;241m=\u001b[39m PruningPasser(\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_, X, missing, y, sample_weight,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m--> 802\u001b[0m \u001b[43mpruning_passer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m imp \u001b[38;5;241m=\u001b[39m pruning_passer\u001b[38;5;241m.\u001b[39mfeature_importance\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_importances_dict \u001b[38;5;241m=\u001b[39m imp\n",
      "File \u001b[1;32mpyearth\\_pruning.pyx:47\u001b[0m, in \u001b[0;36mpyearth._pruning.PruningPasser.run\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpyearth\\_pruning.pyx:144\u001b[0m, in \u001b[0;36mpyearth._pruning.PruningPasser.run\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\XAI_IDS\\venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:2326\u001b[0m, in \u001b[0;36mlstsq\u001b[1;34m(a, b, rcond)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_rhs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2324\u001b[0m     \u001b[38;5;66;03m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m     b \u001b[38;5;241m=\u001b[39m zeros(b\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (m, n_rhs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 2326\u001b[0m x, resids, rank, s \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2328\u001b[0m     x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# MARS surrogate\n",
    "from pyearth import Earth\n",
    "from pwla_lime.pwla_mars import MultiRegionLimeTabularExplainer3D\n",
    "\n",
    "def do_experiment(Xtr, classifier, KERNEL_SIZE, plot_sample_data=False):\n",
    "  from scipy.special import gammainccinv, erfinv\n",
    "  sample_size_lime = 5000\n",
    "  sample_size_lemon = 5000\n",
    "\n",
    "  DIMENSIONS = Xtr.shape[1]\n",
    "  p = 0.999\n",
    "\n",
    "  if type(KERNEL_SIZE) is LambdaType:\n",
    "    KERNEL_SIZE = KERNEL_SIZE(DIMENSIONS)\n",
    "\n",
    "  kernel_width_lime = KERNEL_SIZE\n",
    "  radius = KERNEL_SIZE * np.sqrt(2*gammainccinv(DIMENSIONS/2, (1-p)))\n",
    "  kernel_width_lemon = KERNEL_SIZE\n",
    "  kernel_width_pwla = KERNEL_SIZE\n",
    "  diameter = radius * 2\n",
    "\n",
    "  # explainers\n",
    "  surrogate_lime = Ridge(fit_intercept=True, random_state=123)\n",
    "  # surrogate_pwla = Earth(max_degree=1)          # piecewise-linear\n",
    "\n",
    "  explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
    "      Xtr,\n",
    "      kernel_width=kernel_width_lime,\n",
    "      feature_selection='none',\n",
    "      sample_around_instance=True, # Important! Essentially: True = synthetic, False = observation-based sampling\n",
    "      discretize_continuous=False\n",
    "  )\n",
    "\n",
    "  surrogate_lemon = Earth(max_degree=1)\n",
    "  # surrogate_pwla = Ridge(fit_intercept=True, random_state=123)\n",
    "\n",
    "  # decision tree of maximum depth 3\n",
    "  from sklearn.tree import DecisionTreeRegressor\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "  # add to surrogate_lemon\n",
    "  # surrogate_pwla = DecisionTreeRegressor(max_depth=3, random_state=123)\n",
    "  # surrogate_pwla = DecisionTreeClassifier(max_depth=3, random_state=123)\n",
    "  # plot decision tree after training\n",
    "  # sdsd\n",
    "\n",
    "  # # Initialize multi-region LIME explainer\n",
    "  # pwla_explainer = MultiRegionLimeTabularExplainer3D(\n",
    "  #     Xtr,\n",
    "  #     kernel_width=kernel_width_pwla,\n",
    "  #     feature_selection='none',\n",
    "  #     sample_around_instance=True, # Important! Essentially: True = synthetic, False = observation-based sampling\n",
    "  #     discretize_continuous=False,\n",
    "  # )\n",
    "\n",
    "\n",
    "\n",
    "  explainer_lemon = LemonExplainer(\n",
    "      Xtr,\n",
    "      sample_size=sample_size_lemon,\n",
    "      distance_kernel=partial(gaussian_kernel, kernel_width=kernel_width_lemon),\n",
    "      radius_max=radius\n",
    "  )\n",
    "\n",
    "  Xeval = Xtr\n",
    "  yeval = classifier.predict_proba(Xeval)\n",
    "  ceval = classifier.predict(Xeval)\n",
    "  scaled_data = explainer_lime.scaler.transform(Xeval)\n",
    "\n",
    "  points = []\n",
    "  points2 = []\n",
    "\n",
    "  # for many test instances\n",
    "  for i in range(0, Xeval.shape[0]):\n",
    "      testInstance = (Xeval[i,:], classifier.predict(Xeval[i,:].reshape(1, -1))[0])\n",
    "      # print(\"Test instance:\", i, testInstance)\n",
    "\n",
    "      explanation_lime = explainer_lime.explain_instance(\n",
    "          testInstance[0],\n",
    "          classifier.predict_proba,\n",
    "          num_samples=sample_size_lime,\n",
    "          labels=(testInstance[1],),\n",
    "          model_regressor=surrogate_lime\n",
    "      )\n",
    "\n",
    "\n",
    "      #     # Generate explanation\n",
    "      # pwla_explanation = pwla_explainer.explain_instance(\n",
    "      #     testInstance[0], classifier.predict_proba, num_samples=sample_size_lime, labels=(testInstance[1],),model_regressor=surrogate_pwla\n",
    "      # )\n",
    "\n",
    "      explanation_lemon = explainer_lemon.explain_instance(\n",
    "          testInstance[0],\n",
    "          classifier.predict_proba,\n",
    "          labels=(testInstance[1],),\n",
    "          surrogate=surrogate_lemon\n",
    "      )\n",
    "\n",
    "      ## Misleading: self-reported scores from LIME and LEMON\n",
    "      # lime_fidelity = explanation_lime.score\n",
    "      # lemon_fidelity = explanation_lemon[0][1]\n",
    "\n",
    "      ## Generate 50000 new samples in the neighborhood to compute R^2 between reference and surrogate models.\n",
    "      sample_size = 50000\n",
    "      dimensions = DIMENSIONS\n",
    "      sphere = np.random.normal(size=(sample_size, dimensions))\n",
    "      sphere = normalize(sphere)\n",
    "      sphere *= explainer_lemon.distance_kernel(np.random.uniform(size=sample_size)).reshape(-1,1)\n",
    "      X_transfer = explainer_lemon.scaler.inverse_transform(sphere) + np.array([testInstance[0]])\n",
    "      y_transfer = classifier.predict_proba(X_transfer)\n",
    "      scaled_lime = (X_transfer - explainer_lime.scaler.mean_) / explainer_lime.scaler.scale_\n",
    "      # scaled_pwla = (X_transfer - pwla_explainer.scaler.mean_) / pwla_explainer.scaler.scale_\n",
    "\n",
    "\n",
    "      # from sklearn.metrics import mean_squared_error\n",
    "      from sklearn.metrics import root_mean_squared_error\n",
    "      pwla_fidelity = root_mean_squared_error(\n",
    "        y_transfer[:,testInstance[1]],\n",
    "        surrogate_lemon.predict(X_transfer), # X_transfer\n",
    "        # squared=False\n",
    "      )\n",
    "\n",
    "      lime_fidelity = root_mean_squared_error(\n",
    "        y_transfer[:,testInstance[1]],\n",
    "        surrogate_lime.predict(scaled_lime),\n",
    "        # squared=False\n",
    "      )\n",
    "\n",
    "      points.append([i, pwla_fidelity])\n",
    "      points2.append([i, lime_fidelity])\n",
    "\n",
    "  return np.array(points), np.array(points2)\n",
    "\n",
    "\n",
    "## Sanity check (This requires patching LIME! See previous cell.)\n",
    "# data = Dataset.IRIS.instance\n",
    "# default_rng(seed=2).shuffle(data.data)\n",
    "# Xtr, Xte, ytr, yte = train_test_split(data.data, data.target, test_size=0.6, random_state=123)\n",
    "# classifier = Classifier.GNB.instance\n",
    "# classifier.fit(Xtr, ytr)\n",
    "# do_experiment(Xtr, classifier, 0.2, plot_sample_data=True)\n",
    "\n",
    "data = Dataset.BC.instance\n",
    "# default_rng(seed=2).shuffle(data.data)\n",
    "Xtr = data.data\n",
    "# Xtr, Xte, ytr, yte = train_test_split(data.data, data.target, test_size=0.6, random_state=123)\n",
    "\n",
    "classifier = Classifier.RF.instance\n",
    "classifier.fit(Xtr, data.target)\n",
    "print(Xtr.shape[0])\n",
    "points, points2 = do_experiment(Xtr, classifier, 0.1 , plot_sample_data=False)\n",
    "score_pwla = np.mean(points[:,1])\n",
    "score_lime = np.mean(points2[:,1])\n",
    "print(\"PWLA score:\", score_pwla)\n",
    "print(\"LIME score:\", score_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772ada7f6c0f4ccd8e0e3f5779ac071e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837d7cdc109a4a0eb7505e698fd18239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classifiers:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14883ad5a0a54e9c82b231633138574f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "kernel_widths:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(Xtr, data\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kernel_width \u001b[38;5;129;01min\u001b[39;00m tqdm(kernel_widths, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel_widths\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     50\u001b[0m   \u001b[38;5;66;03m#print(dataset, classifier, kernel_width)\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m   points, points2 \u001b[38;5;241m=\u001b[39m \u001b[43mdo_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_sample_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m   score_lemon \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(points[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     53\u001b[0m   score_lime \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(points2[:,\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[9], line 94\u001b[0m, in \u001b[0;36mdo_experiment\u001b[1;34m(Xtr, classifier, KERNEL_SIZE, plot_sample_data)\u001b[0m\n\u001b[0;32m     80\u001b[0m explanation_lime \u001b[38;5;241m=\u001b[39m explainer_lime\u001b[38;5;241m.\u001b[39mexplain_instance(\n\u001b[0;32m     81\u001b[0m     testInstance[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     82\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mpredict_proba,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     model_regressor\u001b[38;5;241m=\u001b[39msurrogate_lime\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m#     # Generate explanation\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# pwla_explanation = pwla_explainer.explain_instance(\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#     testInstance[0], classifier.predict_proba, num_samples=sample_size_lime, labels=(testInstance[1],),model_regressor=surrogate_pwla\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m explanation_lemon \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer_lemon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestInstance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestInstance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43msurrogate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msurrogate_lemon\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m## Misleading: self-reported scores from LIME and LEMON\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# lime_fidelity = explanation_lime.score\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# lemon_fidelity = explanation_lemon[0][1]\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m## Generate 50000 new samples in the neighborhood to compute R^2 between reference and surrogate models.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 58\u001b[0m, in \u001b[0;36mLemonExplainer.explain_instance\u001b[1;34m(self, instance, predict_fn, labels, surrogate)\u001b[0m\n\u001b[0;32m     55\u001b[0m   score \u001b[38;5;241m=\u001b[39m surrogate\u001b[38;5;241m.\u001b[39mscore(X_transfer, y_transfer[:,label])\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (surrogate\u001b[38;5;241m.\u001b[39mcoef_, score)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [explain_label(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "Cell \u001b[1;32mIn[7], line 58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     55\u001b[0m   score \u001b[38;5;241m=\u001b[39m surrogate\u001b[38;5;241m.\u001b[39mscore(X_transfer, y_transfer[:,label])\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (surrogate\u001b[38;5;241m.\u001b[39mcoef_, score)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mexplain_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "Cell \u001b[1;32mIn[7], line 54\u001b[0m, in \u001b[0;36mLemonExplainer.explain_instance.<locals>.explain_label\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexplain_label\u001b[39m(label):\n\u001b[1;32m---> 54\u001b[0m   \u001b[43msurrogate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_transfer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_transfer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m   score \u001b[38;5;241m=\u001b[39m surrogate\u001b[38;5;241m.\u001b[39mscore(X_transfer, y_transfer[:,label])\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (surrogate\u001b[38;5;241m.\u001b[39mcoef_, score)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\XAI_IDS\\venv\\lib\\site-packages\\sklearn_contrib_py_earth-0+untagged.22.gc4dacee.dirty-py3.9-win-amd64.egg\\pyearth\\earth.py:618\u001b[0m, in \u001b[0;36mEarth.fit\u001b[1;34m(self, X, y, sample_weight, output_weight, missing, xlabels, linvars)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_pass(X, y,\n\u001b[0;32m    615\u001b[0m                   sample_weight, output_weight, missing,\n\u001b[0;32m    616\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxlabels_, linvars, skip_scrub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_pruning \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpruning_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mskip_scrub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmooth\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth:\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_\u001b[38;5;241m.\u001b[39msmooth(X)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\XAI_IDS\\venv\\lib\\site-packages\\sklearn_contrib_py_earth-0+untagged.22.gc4dacee.dirty-py3.9-win-amd64.egg\\pyearth\\earth.py:802\u001b[0m, in \u001b[0;36mEarth.pruning_pass\u001b[1;34m(self, X, y, sample_weight, output_weight, missing, skip_scrub)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;66;03m# Do the actual work\u001b[39;00m\n\u001b[0;32m    799\u001b[0m pruning_passer \u001b[38;5;241m=\u001b[39m PruningPasser(\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_, X, missing, y, sample_weight,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m--> 802\u001b[0m \u001b[43mpruning_passer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m imp \u001b[38;5;241m=\u001b[39m pruning_passer\u001b[38;5;241m.\u001b[39mfeature_importance\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_importances_dict \u001b[38;5;241m=\u001b[39m imp\n",
      "File \u001b[1;32mpyearth\\_pruning.pyx:47\u001b[0m, in \u001b[0;36mpyearth._pruning.PruningPasser.run\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpyearth\\_pruning.pyx:144\u001b[0m, in \u001b[0;36mpyearth._pruning.PruningPasser.run\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\XAI_IDS\\venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:2326\u001b[0m, in \u001b[0;36mlstsq\u001b[1;34m(a, b, rcond)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_rhs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2324\u001b[0m     \u001b[38;5;66;03m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m     b \u001b[38;5;241m=\u001b[39m zeros(b\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (m, n_rhs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 2326\u001b[0m x, resids, rank, s \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2328\u001b[0m     x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Single-core version\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# datasets = [\n",
    "# #  Dataset.IRIS2D,\n",
    "# #   Dataset.WINE,\n",
    "# #   Dataset.PIMA,\n",
    "#   Dataset.BC\n",
    "# ]\n",
    "\n",
    "# classifiers = [\n",
    "# #   Classifier.GNB,\n",
    "# #   Classifier.MLP,\n",
    "#   Classifier.RF\n",
    "# ]\n",
    "\n",
    "# kernel_widths = [\n",
    "#   # 0.1,\n",
    "#   # # 0.2,\n",
    "#   # 0.3,\n",
    "#   # # 0.4,\n",
    "#   # 0.5,\n",
    "#   # # 0.6,\n",
    "#   # # 0.7,\n",
    "#   # # 0.8,\n",
    "#   # # 0.9,\n",
    "#   # 1.0,\n",
    "#   # 1.5,\n",
    "#   # 2.0,\n",
    "#   # 2.5,\n",
    "#   # 3.0,\n",
    "#   # 3.5,\n",
    "#   lambda n: np.sqrt(n) * .75,\n",
    "#   # 4.0\n",
    "# ]\n",
    "\n",
    "# columns = pd.MultiIndex.from_product([datasets, classifiers])\n",
    "# results_lime = pd.DataFrame(0, columns=columns, index=kernel_widths)\n",
    "# results_lemon = pd.DataFrame(0, columns=columns, index=kernel_widths)\n",
    "\n",
    "# for dataset in tqdm(datasets, position=0, desc=\"datasets\", leave=False):\n",
    "#   for classifier in tqdm(classifiers, position=1, desc=\"classifiers\", leave=False):\n",
    "#     data = dataset.instance\n",
    "#     Xtr = data.data\n",
    "#     clf = classifier.instance\n",
    "#     clf.fit(Xtr, data.target)\n",
    "\n",
    "#     for kernel_width in tqdm(kernel_widths, position=2, desc=\"kernel_widths\", leave=False):\n",
    "#       #print(dataset, classifier, kernel_width)\n",
    "#       points, points2 = do_experiment(Xtr, clf, kernel_width, plot_sample_data=False)\n",
    "#       score_lemon = np.mean(points[:,1])\n",
    "#       score_lime = np.mean(points2[:,1])\n",
    "#       #print(\"Mean R^2 LEMON: \", score_lemon)\n",
    "#       #print(\"Mean R^2 LIME: \", score_lime)\n",
    "#       results_lime.loc[kernel_width, (dataset, classifier)] = score_lime\n",
    "#       results_lemon.loc[kernel_width, (dataset, classifier)] = score_lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360143f2c0154eeba5fbc951adc629bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 82\u001b[0m\n\u001b[0;32m     78\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(product(datasets, classifiers, kernel_widths))\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm_joblib(tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(parameters))) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[1;32m---> 82\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m   result\n",
      "File \u001b[1;32mc:\\Users\\Acer\\XAI_IDS\\venv\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\XAI_IDS\\venv\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\XAI_IDS\\venv\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Multi-core version\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "datasets = [\n",
    "#  Dataset.IRIS2D,\n",
    "  # Dataset.WINE,\n",
    "  # Dataset.PIMA,\n",
    "  Dataset.BC\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "  # Classifier.GNB,\n",
    "  # Classifier.MLP,\n",
    "  Classifier.RF\n",
    "]\n",
    "\n",
    "kernel_widths = [\n",
    "  # 0.1,\n",
    "  # # 0.2,\n",
    "  0.3,\n",
    "  # # 0.4,\n",
    "  # 0.5,\n",
    "  # # 0.6,\n",
    "  # # 0.7,\n",
    "  # # 0.8,\n",
    "  # # 0.9,\n",
    "  # 1.0,\n",
    "  # 1.5,\n",
    "  # 2.0,\n",
    "  # 2.5,\n",
    "  # 3.0,\n",
    "  # 3.5,\n",
    "  # lambda n: np.sqrt(n) * .75,\n",
    "  # 4.0\n",
    "]\n",
    "\n",
    "columns = pd.MultiIndex.from_product([datasets, classifiers])\n",
    "\n",
    "def faithfulness(options):\n",
    "  dataset, classifier, kernel_width = options\n",
    "  data = dataset.instance\n",
    "  Xtr = data.data\n",
    "  clf = classifier.instance\n",
    "  clf.fit(Xtr, data.target)\n",
    "\n",
    "  points, points2 = do_experiment(Xtr, clf, kernel_width, plot_sample_data=False)\n",
    "  score_lemon = np.mean(points[:,1])\n",
    "  score_lime = np.mean(points2[:,1])\n",
    "  return score_lime, score_lemon\n",
    "\n",
    "\n",
    "\n",
    "import contextlib\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "from joblib import delayed, Parallel, cpu_count\n",
    "\n",
    "parameters = list(product(datasets, classifiers, kernel_widths))\n",
    "\n",
    "\n",
    "with tqdm_joblib(tqdm(total=len(parameters))) as progress_bar:\n",
    "  result = Parallel(n_jobs=cpu_count())(delayed(faithfulness)(p) for p in parameters)\n",
    "  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_widths_cleaned = ['0.75*sqrt(n)' if type(k) is LambdaType else k for k in kernel_widths]\n",
    "results_lime = pd.DataFrame(0, columns=columns, index=kernel_widths_cleaned)\n",
    "results_pwla = pd.DataFrame(0, columns=columns, index=kernel_widths_cleaned)\n",
    "for i, (dataset, classifier, kernel_width) in enumerate(parameters):\n",
    "  res = result[i]\n",
    "  if res is None:\n",
    "      # either skip, or fill with NaN\n",
    "      continue\n",
    "  # score_lime, score_lemon = res\n",
    "  score_lime = result[i][0]\n",
    "  score_pwla = result[i][1]\n",
    "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
    "  results_pwla.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_pwla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pwla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean((results_lime - results_pwla) / results_lime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
