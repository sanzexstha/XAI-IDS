{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:34:01.372127Z",
     "start_time": "2019-01-16T10:33:59.687460Z"
    },
    "code_folding": [
     21,
     82
    ]
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "from sklearn.utils import Bunch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.linear_model import Ridge\n",
    "import random\n",
    "from scipy.special import gammainccinv, erfinv\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import chi2, norm\n",
    "from numpy.random import default_rng\n",
    "from types import LambdaType\n",
    "\n",
    "\n",
    "class Dataset(Enum):\n",
    "  GENERATED = 'Generated'\n",
    "  BLOBS = 'Blobs'\n",
    "  IRIS2D = 'Iris (2D)'\n",
    "  IRIS = 'Iris'\n",
    "  BC = 'Breast cancer'\n",
    "  PIMA = 'Pima Indian'\n",
    "  WINE = 'Wine'\n",
    "\n",
    "  @property\n",
    "  def instance(self):\n",
    "    if self == Dataset.GENERATED:\n",
    "      amount = 2500\n",
    "      data = np.random.uniform(0,[3,10],(amount,2))\n",
    "      return Bunch(\n",
    "        data=data,\n",
    "        feature_names=['feature 1', 'feature 2'],\n",
    "        target_names=['yellow', 'blue'],\n",
    "        target=(np.logical_and(data[:,0]>1, data[:,1]>2)).astype(int)\n",
    "      )\n",
    "\n",
    "    if self == Dataset.BLOBS:\n",
    "      from sklearn.datasets import make_blobs\n",
    "      amount = 250\n",
    "      X,y = make_blobs(amount, n_features = 3, centers = 4)\n",
    "      return Bunch(\n",
    "        data=X,\n",
    "        feature_names=['feature 1', 'feature 2', 'feature 3'],\n",
    "        target_names=['yellow', 'blue'],\n",
    "        target=(y>1).astype(int)\n",
    "      )\n",
    "\n",
    "    if self == Dataset.IRIS2D:\n",
    "      from sklearn.datasets import load_iris\n",
    "      dataset = load_iris()\n",
    "      indexes = np.array([0,1])\n",
    "      dataset.data = dataset.data[:, indexes]\n",
    "      dataset.feature_names = np.array(dataset.feature_names)[indexes]\n",
    "      return dataset\n",
    "\n",
    "    if self == Dataset.IRIS:\n",
    "      from sklearn.datasets import load_iris\n",
    "      return load_iris()\n",
    "\n",
    "    if self == Dataset.BC:\n",
    "      from sklearn.datasets import load_breast_cancer\n",
    "      dataset = load_breast_cancer()\n",
    "      return dataset\n",
    "\n",
    "    if self == Dataset.WINE:\n",
    "      from sklearn.datasets import load_wine\n",
    "      dataset = load_wine()\n",
    "      return dataset\n",
    "\n",
    "    if self == Dataset.PIMA:\n",
    "      from sklearn.datasets import load_diabetes\n",
    "      dataset = load_diabetes()\n",
    "      dataset.target = (dataset.target > 200).astype(int)\n",
    "      return dataset\n",
    "\n",
    "\n",
    "class Classifier(Enum):\n",
    "  DT = 'Decision tree'\n",
    "  RF = 'Random Forest'\n",
    "  MLP = 'Perceptron'\n",
    "  GNB = 'Naive Bayes'\n",
    "  SVM = 'Support vector machine'\n",
    "  QDA = 'Quadratic Discriminant Analysis'\n",
    "\n",
    "  @property\n",
    "  def instance(self):\n",
    "    if self == Classifier.DT:\n",
    "      from sklearn.tree import DecisionTreeClassifier\n",
    "      return DecisionTreeClassifier(random_state=1)\n",
    "    if self == Classifier.RF:\n",
    "      from sklearn.ensemble import RandomForestClassifier\n",
    "      return RandomForestClassifier(random_state=1, n_estimators=200)\n",
    "    elif self == Classifier.MLP:\n",
    "      from sklearn.neural_network import MLPClassifier\n",
    "      return MLPClassifier(alpha=0.1, hidden_layer_sizes=(100,100,100))\n",
    "    elif self == Classifier.GNB:\n",
    "      from sklearn.naive_bayes import GaussianNB\n",
    "      return GaussianNB()\n",
    "    elif self == Classifier.SVM:\n",
    "      from sklearn.svm import SVC\n",
    "      return SVC(probability=True)\n",
    "    elif self == Classifier.QDA:\n",
    "      from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "      return QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T11:06:12.727852Z",
     "start_time": "2019-01-16T11:06:12.702853Z"
    },
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from bisect import bisect\n",
    "from scipy.optimize import newton\n",
    "from functools import partial\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "class LemonExplainer(object):\n",
    "  \"\"\"\n",
    "  Intantiates the explainer.\n",
    "  \"\"\"\n",
    "  def __init__(self, training_data, distance_kernel=None, sample_size = 5000, radius_max=1, random_state=None):\n",
    "    self.random_state = check_random_state(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    self.training_data = training_data\n",
    "    self.scaler = StandardScaler(with_mean=False)\n",
    "    self.scaler.fit(training_data)\n",
    "\n",
    "    # Create hypersphere samples. The sphere is only computed once for performance and stability,\n",
    "    # but it would be better to resample the sphere every time `explain_instance` is called.\n",
    "    # I checked, this does not affect the results in any way.\n",
    "    dimensions = training_data.shape[1]\n",
    "\n",
    "    if distance_kernel is None:\n",
    "      self.distance_kernel = np.vectorize(lambda x: x ** (1 / dimensions))\n",
    "    else:\n",
    "      self.distance_kernel = np.vectorize(self._transform(distance_kernel, dimensions, radius_max=radius_max))\n",
    "\n",
    "    sphere = np.random.normal(size=(sample_size, dimensions))\n",
    "    sphere = normalize(sphere)\n",
    "    sphere *= self.distance_kernel(np.random.uniform(size=sample_size)).reshape(-1,1)\n",
    "\n",
    "    self.sphere = sphere\n",
    "\n",
    "  @property\n",
    "  def surrogate(self):\n",
    "    try:\n",
    "      return self._surrgate\n",
    "    except AttributeError:\n",
    "      self._surrogate = Ridge(alpha=0, fit_intercept=True, normalize=True, random_state=self.random_state)\n",
    "      return self._surrogate\n",
    "\n",
    "  def explain_instance(self, instance, predict_fn, labels=(1,), surrogate=None):\n",
    "    surrogate = surrogate or self.surrogate\n",
    "\n",
    "    # Create transfer dataset by perturbing the original instance with the hypersphere samples\n",
    "    X_transfer = self.scaler.inverse_transform(self.sphere) + np.array([instance])\n",
    "    y_transfer = predict_fn(X_transfer)\n",
    "\n",
    "    def explain_label(label):\n",
    "      surrogate.fit(X_transfer, y_transfer[:,label])\n",
    "      score = surrogate.score(X_transfer, y_transfer[:,label])\n",
    "      return (surrogate.coef_, score)\n",
    "\n",
    "    return [explain_label(label) for label in labels]\n",
    "\n",
    "  def _transform(self, kernel, dimensions, sample_size = 1000, radius_max=1):\n",
    "    \"\"\"\n",
    "    Inverse transform sampling\n",
    "    \"\"\"\n",
    "    cdf_samples = np.array([kernel(x)*(x**(dimensions-1)) for x in np.linspace(0, radius_max, sample_size)])\n",
    "    cdf_samples = np.cumsum(cdf_samples)\n",
    "    cdf_samples /= cdf_samples[-1]\n",
    "    return lambda y: radius_max * (bisect(cdf_samples, y) / sample_size)\n",
    "\n",
    "\n",
    "def uniform_kernel(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def gaussian_kernel(x, kernel_width):\n",
    "    # https://github.com/marcotcr/lime/blob/fd7eb2e6f760619c29fca0187c07b82157601b32/lime/lime_tabular.py#L251\n",
    "    return np.sqrt(np.exp(-(x ** 2) / kernel_width ** 2))\n",
    "\n",
    "\n",
    "def sqcos_kernel(x):\n",
    "    return np.cos(x)**2\n",
    "\n",
    "\n",
    "def trapezoid_kernel(x, a, b):\n",
    "    if 0 <= x and  x <= a:\n",
    "        return (2 / (a + b))\n",
    "    elif a <= x and x <= b:\n",
    "        return (2 / (a + b)) * ((b - x) / (b - a))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 1.1.3\n"
     ]
    }
   ],
   "source": [
    "# For the `plot_sample_data` debug feature in the next cell, you need to patch LIME as such:\n",
    "\n",
    "# Change return statement from `explain_instance_with_data` (lime_base.py:204) to\n",
    "\n",
    "#         return (easy_model.intercept_,\n",
    "#                 sorted(zip(used_features, easy_model.coef_),\n",
    "#                        key=lambda x: np.abs(x[1]), reverse=True),\n",
    "#                 prediction_score, local_pred, neighborhood_data[:, used_features], weights)\n",
    "\n",
    "# And `ret_exp` in `explain_instance` (lime_tabular.py:450) to\n",
    "\n",
    "#             (ret_exp.intercept[label],\n",
    "#              ret_exp.local_exp[label],\n",
    "#              ret_exp.score, ret_exp.local_pred, ret_exp.data, ret_exp.weights) = self.base.explain_instance_with_data(\n",
    "#                     scaled_data,\n",
    "#                     yss,\n",
    "#                     distances,\n",
    "#                     label,\n",
    "#                     num_features,\n",
    "#                     model_regressor=model_regressor,\n",
    "#                     feature_selection=self.feature_selection)\n",
    "\n",
    "#print sklearn.__version__\n",
    "import sklearn\n",
    "\n",
    "\n",
    "print(\"sklearn version:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T11:06:20.844684Z",
     "start_time": "2019-01-16T11:06:13.964182Z"
    }
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This Ridge instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m classifier \u001b[38;5;241m=\u001b[39m Classifier\u001b[38;5;241m.\u001b[39mGNB\u001b[38;5;241m.\u001b[39minstance\n\u001b[0;32m    189\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(Xtr, ytr)\n\u001b[1;32m--> 190\u001b[0m \u001b[43mdo_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_sample_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 78\u001b[0m, in \u001b[0;36mdo_experiment\u001b[1;34m(Xtr, classifier, KERNEL_SIZE, plot_sample_data)\u001b[0m\n\u001b[0;32m     73\u001b[0m y_transfer \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict_proba(X_transfer)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m     76\u001b[0m lemon_fidelity \u001b[38;5;241m=\u001b[39m mean_squared_error(\n\u001b[0;32m     77\u001b[0m   y_transfer[:,testInstance[\u001b[38;5;241m1\u001b[39m]],\n\u001b[1;32m---> 78\u001b[0m   \u001b[43msurrogate_lemon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_transfer\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     79\u001b[0m   squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     80\u001b[0m )\n\u001b[0;32m     82\u001b[0m scaled \u001b[38;5;241m=\u001b[39m (X_transfer \u001b[38;5;241m-\u001b[39m explainer_lime\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mmean_) \u001b[38;5;241m/\u001b[39m explainer_lime\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m     83\u001b[0m lime_fidelity \u001b[38;5;241m=\u001b[39m mean_squared_error(\n\u001b[0;32m     84\u001b[0m   y_transfer[:,testInstance[\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m     85\u001b[0m   surrogate_lime\u001b[38;5;241m.\u001b[39mpredict(scaled),\n\u001b[0;32m     86\u001b[0m   squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     87\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:367\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m--> 367\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1345\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1341\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1342\u001b[0m     ]\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[1;32m-> 1345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This Ridge instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "def do_experiment(Xtr, classifier, KERNEL_SIZE, plot_sample_data=False):\n",
    "  from scipy.special import gammainccinv, erfinv\n",
    "  sample_size_lime = 5000\n",
    "  sample_size_lemon = 5000\n",
    "\n",
    "  DIMENSIONS = Xtr.shape[1]\n",
    "  p = 0.999\n",
    "\n",
    "  if type(KERNEL_SIZE) is LambdaType:\n",
    "    KERNEL_SIZE = KERNEL_SIZE(DIMENSIONS)\n",
    "\n",
    "  kernel_width_lime = KERNEL_SIZE\n",
    "  radius = KERNEL_SIZE * np.sqrt(2*gammainccinv(DIMENSIONS/2, (1-p)))\n",
    "  kernel_width_lemon = KERNEL_SIZE\n",
    "  diameter = radius * 2\n",
    "\n",
    "  # explainers\n",
    "  surrogate_lime = Ridge(fit_intercept=True, random_state=123)\n",
    "  explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
    "      Xtr,\n",
    "      # kernel_width=kernel_width_lime,\n",
    "      feature_selection='none',\n",
    "      sample_around_instance=True, # Important! Essentially: True = synthetic, False = observation-based sampling\n",
    "      discretize_continuous=False\n",
    "  )\n",
    "\n",
    "  surrogate_lemon = Ridge(fit_intercept=True, random_state=123)\n",
    "  # explainer_lemon = LemonExplainer(\n",
    "  #     Xtr,\n",
    "  #     sample_size=sample_size_lemon,\n",
    "  #     distance_kernel=partial(gaussian_kernel, kernel_width=kernel_width_lemon),\n",
    "  #     radius_max=radius\n",
    "  # )\n",
    "\n",
    "  Xeval = Xtr\n",
    "  yeval = classifier.predict_proba(Xeval)\n",
    "  ceval = classifier.predict(Xeval)\n",
    "  scaled_data = explainer_lime.scaler.transform(Xeval)\n",
    "\n",
    "  points = []\n",
    "  points2 = []\n",
    "\n",
    "  # for many test instances\n",
    "  for i in range(0, Xeval.shape[0]):\n",
    "      testInstance = (Xeval[i,:], classifier.predict(Xeval[i,:].reshape(1, -1))[0])\n",
    "\n",
    "      explanation_lime = explainer_lime.explain_instance(\n",
    "          testInstance[0],\n",
    "          classifier.predict_proba,\n",
    "          num_samples=sample_size_lime,\n",
    "          labels=(testInstance[1],),\n",
    "          model_regressor=surrogate_lime\n",
    "      )\n",
    "\n",
    "      # explanation_lemon = explainer_lemon.explain_instance(\n",
    "      #     testInstance[0],\n",
    "      #     classifier.predict_proba,\n",
    "      #     labels=(testInstance[1],),\n",
    "      #     surrogate=surrogate_lemon\n",
    "      # )\n",
    "\n",
    "      ## Misleading: self-reported scores from LIME and LEMON\n",
    "      # lime_fidelity = explanation_lime.score\n",
    "      # lemon_fidelity = explanation_lemon[0][1]\n",
    "\n",
    "      ## Generate 50000 new samples in the neighborhood to compute R^2 between reference and surrogate models.\n",
    "      sample_size = 50000\n",
    "      dimensions = DIMENSIONS\n",
    "      sphere = np.random.normal(size=(sample_size, dimensions))\n",
    "      sphere = normalize(sphere)\n",
    "      sphere *= explainer_lemon.distance_kernel(np.random.uniform(size=sample_size)).reshape(-1,1)\n",
    "      X_transfer = explainer_lemon.scaler.inverse_transform(sphere) + np.array([testInstance[0]])\n",
    "      y_transfer = classifier.predict_proba(X_transfer)\n",
    "\n",
    "      from sklearn.metrics import mean_squared_error\n",
    "      lemon_fidelity = mean_squared_error(\n",
    "        y_transfer[:,testInstance[1]],\n",
    "        surrogate_lemon.predict(X_transfer),\n",
    "        squared=False\n",
    "      )\n",
    "\n",
    "      scaled = (X_transfer - explainer_lime.scaler.mean_) / explainer_lime.scaler.scale_\n",
    "      lime_fidelity = mean_squared_error(\n",
    "        y_transfer[:,testInstance[1]],\n",
    "        surrogate_lime.predict(scaled),\n",
    "        squared=False\n",
    "      )\n",
    "\n",
    "      points.append([i, lemon_fidelity])\n",
    "      points2.append([i, lime_fidelity])\n",
    "\n",
    "      # Plot sampled points\n",
    "      if plot_sample_data:\n",
    "        from matplotlib.patches import Ellipse\n",
    "        slice = (0,1)\n",
    "\n",
    "        # Evaluation data\n",
    "        if 'sphere' in vars():\n",
    "          # Evaluation data (labeled by reference model)\n",
    "          plt.figure(figsize=(5, 5))\n",
    "          plt.xlim(np.min(Xtr[:,slice[0]]) + 1, np.max(Xtr[:,slice[0]]) + 1)\n",
    "          plt.ylim(np.min(Xtr[:,slice[1]]), np.max(Xtr[:,slice[1]]))\n",
    "\n",
    "          plt.scatter(X_transfer[:,slice[0]], X_transfer[:,slice[1]], cmap='coolwarm', s=0.005, c=y_transfer[:,1])\n",
    "          plt.title(\"Evaluation data (labeled by reference model)\")\n",
    "\n",
    "          w, h = explainer_lemon.scaler.inverse_transform([np.repeat(diameter, DIMENSIONS)])[0, slice]\n",
    "          plot = plt.scatter(testInstance[0][slice[0]], testInstance[0][slice[1]], s=100, marker='s')\n",
    "          circle = Ellipse((testInstance[0][slice[0]], testInstance[0][slice[1]]), width=w, height=h, color='r', fill=False)\n",
    "          plot.axes.add_artist(circle)\n",
    "          plt.show()\n",
    "\n",
    "          # Evaluation data (labeled by LIME surrogate)\n",
    "          plt.figure(figsize=(5, 5))\n",
    "          plt.xlim(np.min(Xtr[:,slice[0]]) + 1, np.max(Xtr[:,slice[0]]) + 1)\n",
    "          plt.ylim(np.min(Xtr[:,slice[1]]), np.max(Xtr[:,slice[1]]))\n",
    "\n",
    "          plt.scatter(X_transfer[:,slice[0]], X_transfer[:,slice[1]], cmap='coolwarm', s=0.005, c=surrogate_lime.predict((X_transfer - explainer_lime.scaler.mean_) / explainer_lime.scaler.scale_))\n",
    "          plt.title(\"Evaluation data (labeled by LIME surrogate)\")\n",
    "\n",
    "          w, h = explainer_lemon.scaler.inverse_transform([np.repeat(diameter, DIMENSIONS)])[0, slice]\n",
    "          plot = plt.scatter(testInstance[0][slice[0]], testInstance[0][slice[1]], s=100, marker='s')\n",
    "          circle = Ellipse((testInstance[0][slice[0]], testInstance[0][slice[1]]), width=w, height=h, color='r', fill=False)\n",
    "          plot.axes.add_artist(circle)\n",
    "          plt.show()\n",
    "\n",
    "          # Evaluation data (labeled by LEMON surrogate)\n",
    "          plt.figure(figsize=(5, 5))\n",
    "          plt.xlim(np.min(Xtr[:,slice[0]]) + 1, np.max(Xtr[:,slice[0]]) + 1)\n",
    "          plt.ylim(np.min(Xtr[:,slice[1]]), np.max(Xtr[:,slice[1]]))\n",
    "\n",
    "          plt.scatter(X_transfer[:,slice[0]], X_transfer[:,slice[1]], cmap='coolwarm', s=0.005, c=surrogate_lemon.predict(X_transfer))\n",
    "          plt.title(\"Evaluation data (labeled by LEMON surrogate)\")\n",
    "\n",
    "          w, h = explainer_lemon.scaler.inverse_transform([np.repeat(diameter, DIMENSIONS)])[0, slice]\n",
    "          plot = plt.scatter(testInstance[0][slice[0]], testInstance[0][slice[1]], s=100, marker='s')\n",
    "          circle = Ellipse((testInstance[0][slice[0]], testInstance[0][slice[1]]), width=w, height=h, color='r', fill=False)\n",
    "          plot.axes.add_artist(circle)\n",
    "          plt.show()\n",
    "\n",
    "        # LIME\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.xlim(np.min(Xtr[:,slice[0]]) + 1, np.max(Xtr[:,slice[0]]) + 1)\n",
    "        plt.ylim(np.min(Xtr[:,slice[1]]), np.max(Xtr[:,slice[1]]))\n",
    "\n",
    "        data = (explanation_lime.data * explainer_lime.scaler.scale_) + explainer_lime.scaler.mean_\n",
    "        ax = plt.scatter(data[:,slice[0]], data[:,slice[1]], cmap='coolwarm', s=np.around(explanation_lime.weights*10, decimals=7), c=surrogate_lime.predict(explanation_lime.data))\n",
    "        plt.title(\"LIME transfer data\")\n",
    "\n",
    "        w, h = explainer_lime.scaler.inverse_transform([np.repeat(diameter, DIMENSIONS)])[0, slice]\n",
    "        plot = plt.scatter(testInstance[0][slice[0]], testInstance[0][slice[1]], s=100, marker='s' )\n",
    "        circle = Ellipse((testInstance[0][slice[0]], testInstance[0][slice[1]]), width=w, height=h, color='r', fill=False)\n",
    "        plot.axes.add_artist(circle)\n",
    "        plt.show()\n",
    "\n",
    "        # LEMON\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.xlim(np.min(Xtr[:,slice[0]]) + 1, np.max(Xtr[:,slice[0]]) + 1)\n",
    "        plt.ylim(np.min(Xtr[:,slice[1]]), np.max(Xtr[:,slice[1]]))\n",
    "\n",
    "        X_transfer = explainer_lemon.scaler.inverse_transform(explainer_lemon.sphere) + np.array([testInstance[0]])\n",
    "        plt.scatter(X_transfer[:,slice[0]], X_transfer[:,slice[1]], cmap='coolwarm', s=1, c=1 - surrogate_lemon.predict(X_transfer))\n",
    "        plt.title(\"LEMON transfer data\")\n",
    "\n",
    "        w, h = explainer_lemon.scaler.inverse_transform([np.repeat(diameter, DIMENSIONS)])[0, slice]\n",
    "        plot = plt.scatter(testInstance[0][slice[0]], testInstance[0][slice[1]], s=100, marker='s')\n",
    "        circle = Ellipse((testInstance[0][slice[0]], testInstance[0][slice[1]]), width=w, height=h, color='r', fill=False)\n",
    "        plot.axes.add_artist(circle)\n",
    "        plt.show()\n",
    "\n",
    "        # return early\n",
    "        return np.array(points), np.array(points2)\n",
    "\n",
    "  return np.array(points), np.array(points2)\n",
    "\n",
    "\n",
    "## Sanity check (This requires patching LIME! See previous cell.)\n",
    "# data = Dataset.IRIS.instance\n",
    "# default_rng(seed=2).shuffle(data.data)\n",
    "# Xtr, Xte, ytr, yte = train_test_split(data.data, data.target, test_size=0.6, random_state=123)\n",
    "# classifier = Classifier.GNB.instance\n",
    "# classifier.fit(Xtr, ytr)\n",
    "# do_experiment(Xtr, classifier, 0.2, plot_sample_data=True)\n",
    "\n",
    "data = Dataset.BC.instance\n",
    "default_rng(seed=2).shuffle(data.data)\n",
    "Xtr, Xte, ytr, yte = train_test_split(data.data, data.target, test_size=0.6, random_state=123)\n",
    "classifier = Classifier.GNB.instance\n",
    "classifier.fit(Xtr, ytr)\n",
    "do_experiment(Xtr, classifier, 2, plot_sample_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-core version\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# datasets = [\n",
    "# #  Dataset.IRIS2D,\n",
    "#   Dataset.WINE,\n",
    "#   Dataset.PIMA,\n",
    "#   Dataset.BC\n",
    "# ]\n",
    "\n",
    "# classifiers = [\n",
    "#   Classifier.GNB,\n",
    "#   Classifier.MLP,\n",
    "#   Classifier.RF\n",
    "# ]\n",
    "\n",
    "# kernel_widths = [\n",
    "#   0.1,\n",
    "#   0.2,\n",
    "#   0.3,\n",
    "#   # 0.4,\n",
    "#   0.5,\n",
    "#   # 0.6,\n",
    "#   # 0.7,\n",
    "#   # 0.8,\n",
    "#   # 0.9,\n",
    "#   1.0,\n",
    "#   # 1.5,\n",
    "#   2.0,\n",
    "#   # 2.5,\n",
    "#   # 3.0,\n",
    "#   # 3.5,\n",
    "#   4.0\n",
    "# ]\n",
    "\n",
    "# columns = pd.MultiIndex.from_product([datasets, classifiers])\n",
    "# results_lime = pd.DataFrame(0, columns=columns, index=kernel_widths)\n",
    "# results_lemon = pd.DataFrame(0, columns=columns, index=kernel_widths)\n",
    "\n",
    "# for dataset in tqdm(datasets, position=0, desc=\"datasets\", leave=False):\n",
    "#   for classifier in tqdm(classifiers, position=1, desc=\"classifiers\", leave=False):\n",
    "#     data = dataset.instance\n",
    "#     Xtr = data.data\n",
    "#     clf = classifier.instance\n",
    "#     clf.fit(Xtr, data.target)\n",
    "\n",
    "#     for kernel_width in tqdm(kernel_widths, position=2, desc=\"kernel_widths\", leave=False):\n",
    "#       #print(dataset, classifier, kernel_width)\n",
    "#       points, points2 = do_experiment(Xtr, clf, kernel_width, plot_sample_data=False)\n",
    "#       score_lemon = np.mean(points[:,1])\n",
    "#       score_lime = np.mean(points2[:,1])\n",
    "#       #print(\"Mean R^2 LEMON: \", score_lemon)\n",
    "#       #print(\"Mean R^2 LIME: \", score_lime)\n",
    "#       results_lime.loc[kernel_width, (dataset, classifier)] = score_lime\n",
    "#       results_lemon.loc[kernel_width, (dataset, classifier)] = score_lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284605b28b194ab9b87b38d5b2157662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multi-core version\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "datasets = [\n",
    "#  Dataset.IRIS2D,\n",
    "  Dataset.WINE,\n",
    "  Dataset.PIMA,\n",
    "  Dataset.BC\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "  Classifier.GNB,\n",
    "  Classifier.MLP,\n",
    "  Classifier.RF\n",
    "]\n",
    "\n",
    "kernel_widths = [\n",
    "  0.1,\n",
    "  # 0.2,\n",
    "  0.3,\n",
    "  # 0.4,\n",
    "  0.5,\n",
    "  # 0.6,\n",
    "  # 0.7,\n",
    "  # 0.8,\n",
    "  # 0.9,\n",
    "  1.0,\n",
    "  # 1.5,\n",
    "  # 2.0,\n",
    "  # 2.5,\n",
    "  # 3.0,\n",
    "  # 3.5,\n",
    "  lambda n: np.sqrt(n) * .75,\n",
    "  4.0\n",
    "]\n",
    "\n",
    "columns = pd.MultiIndex.from_product([datasets, classifiers])\n",
    "\n",
    "def faithfulness(options):\n",
    "  dataset, classifier, kernel_width = options\n",
    "  data = dataset.instance\n",
    "  Xtr = data.data\n",
    "  clf = classifier.instance\n",
    "  clf.fit(Xtr, data.target)\n",
    "\n",
    "  points, points2 = do_experiment(Xtr, clf, kernel_width, plot_sample_data=False)\n",
    "  score_lemon = np.mean(points[:,1])\n",
    "  score_lime = np.mean(points2[:,1])\n",
    "\n",
    "  return score_lime, score_lemon\n",
    "\n",
    "\n",
    "import contextlib\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "from joblib import delayed, Parallel, cpu_count\n",
    "\n",
    "parameters = list(product(datasets, classifiers, kernel_widths))\n",
    "\n",
    "\n",
    "with tqdm_joblib(tqdm(total=len(parameters))) as progress_bar:\n",
    "  result = Parallel(n_jobs=cpu_count())(delayed(faithfulness)(p) for p in parameters)\n",
    "  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.00895809849612416' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.0034053433160280916' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.02465593030094842' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.006799356690257597' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.04120760887601594' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.017835233042241407' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.018215244414075748' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.01637221116755501' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.01616695454237722' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.014524305895818615' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.07218649683468607' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.035582384639828865' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.01102685446511626' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.00587473744044511' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.1954438961983432' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.11459846413348412' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.037691604148474035' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29212\\1920169935.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.014777966222518462' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon\n"
     ]
    }
   ],
   "source": [
    "kernel_widths_cleaned = ['0.75*sqrt(n)' if type(k) is LambdaType else k for k in kernel_widths]\n",
    "results_lime = pd.DataFrame(0, columns=columns, index=kernel_widths_cleaned)\n",
    "results_lemon = pd.DataFrame(0, columns=columns, index=kernel_widths_cleaned)\n",
    "for i, (dataset, classifier, kernel_width) in enumerate(parameters):\n",
    "  score_lime = result[i][0]\n",
    "  score_lemon = result[i][1]\n",
    "  results_lime.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lime\n",
    "  results_lemon.loc['0.75*sqrt(n)' if type(kernel_width) is LambdaType else kernel_width, (dataset, classifier)] = score_lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Dataset.WINE</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Dataset.PIMA</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Dataset.BC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Classifier.GNB</th>\n",
       "      <th>Classifier.MLP</th>\n",
       "      <th>Classifier.RF</th>\n",
       "      <th>Classifier.GNB</th>\n",
       "      <th>Classifier.MLP</th>\n",
       "      <th>Classifier.RF</th>\n",
       "      <th>Classifier.GNB</th>\n",
       "      <th>Classifier.MLP</th>\n",
       "      <th>Classifier.RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.008958</td>\n",
       "      <td>0.024656</td>\n",
       "      <td>0.041208</td>\n",
       "      <td>0.018215</td>\n",
       "      <td>0.016167</td>\n",
       "      <td>0.072186</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.195444</td>\n",
       "      <td>0.037692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.043742</td>\n",
       "      <td>0.112717</td>\n",
       "      <td>0.118031</td>\n",
       "      <td>0.057038</td>\n",
       "      <td>0.051196</td>\n",
       "      <td>0.141244</td>\n",
       "      <td>0.051814</td>\n",
       "      <td>0.407754</td>\n",
       "      <td>0.102741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.102591</td>\n",
       "      <td>0.254024</td>\n",
       "      <td>0.185647</td>\n",
       "      <td>0.079695</td>\n",
       "      <td>0.069817</td>\n",
       "      <td>0.111645</td>\n",
       "      <td>0.150783</td>\n",
       "      <td>0.522251</td>\n",
       "      <td>0.170851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.258127</td>\n",
       "      <td>0.227603</td>\n",
       "      <td>0.155471</td>\n",
       "      <td>0.119660</td>\n",
       "      <td>0.065302</td>\n",
       "      <td>0.104605</td>\n",
       "      <td>0.490756</td>\n",
       "      <td>0.573657</td>\n",
       "      <td>0.264988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75*sqrt(n)</th>\n",
       "      <td>0.652039</td>\n",
       "      <td>0.544232</td>\n",
       "      <td>0.375671</td>\n",
       "      <td>0.386784</td>\n",
       "      <td>0.239042</td>\n",
       "      <td>0.246592</td>\n",
       "      <td>0.511616</td>\n",
       "      <td>0.839918</td>\n",
       "      <td>0.366622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.846789</td>\n",
       "      <td>0.891078</td>\n",
       "      <td>0.544923</td>\n",
       "      <td>0.686621</td>\n",
       "      <td>0.473535</td>\n",
       "      <td>0.418620</td>\n",
       "      <td>0.504891</td>\n",
       "      <td>0.618730</td>\n",
       "      <td>0.357678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset.WINE                                Dataset.PIMA  \\\n",
       "             Classifier.GNB Classifier.MLP Classifier.RF Classifier.GNB   \n",
       "0.1                0.008958       0.024656      0.041208       0.018215   \n",
       "0.3                0.043742       0.112717      0.118031       0.057038   \n",
       "0.5                0.102591       0.254024      0.185647       0.079695   \n",
       "1.0                0.258127       0.227603      0.155471       0.119660   \n",
       "0.75*sqrt(n)       0.652039       0.544232      0.375671       0.386784   \n",
       "4.0                0.846789       0.891078      0.544923       0.686621   \n",
       "\n",
       "                                              Dataset.BC                 \\\n",
       "             Classifier.MLP Classifier.RF Classifier.GNB Classifier.MLP   \n",
       "0.1                0.016167      0.072186       0.011027       0.195444   \n",
       "0.3                0.051196      0.141244       0.051814       0.407754   \n",
       "0.5                0.069817      0.111645       0.150783       0.522251   \n",
       "1.0                0.065302      0.104605       0.490756       0.573657   \n",
       "0.75*sqrt(n)       0.239042      0.246592       0.511616       0.839918   \n",
       "4.0                0.473535      0.418620       0.504891       0.618730   \n",
       "\n",
       "                            \n",
       "             Classifier.RF  \n",
       "0.1               0.037692  \n",
       "0.3               0.102741  \n",
       "0.5               0.170851  \n",
       "1.0               0.264988  \n",
       "0.75*sqrt(n)      0.366622  \n",
       "4.0               0.357678  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Dataset.WINE</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Dataset.PIMA</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Dataset.BC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Classifier.GNB</th>\n",
       "      <th>Classifier.MLP</th>\n",
       "      <th>Classifier.RF</th>\n",
       "      <th>Classifier.GNB</th>\n",
       "      <th>Classifier.MLP</th>\n",
       "      <th>Classifier.RF</th>\n",
       "      <th>Classifier.GNB</th>\n",
       "      <th>Classifier.MLP</th>\n",
       "      <th>Classifier.RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.017835</td>\n",
       "      <td>0.016372</td>\n",
       "      <td>0.014524</td>\n",
       "      <td>0.035582</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>0.114598</td>\n",
       "      <td>0.014778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.026427</td>\n",
       "      <td>0.040270</td>\n",
       "      <td>0.050753</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.026401</td>\n",
       "      <td>0.052811</td>\n",
       "      <td>0.030298</td>\n",
       "      <td>0.193765</td>\n",
       "      <td>0.038165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.070718</td>\n",
       "      <td>0.141764</td>\n",
       "      <td>0.081997</td>\n",
       "      <td>0.045729</td>\n",
       "      <td>0.031290</td>\n",
       "      <td>0.064341</td>\n",
       "      <td>0.104709</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.057385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.223794</td>\n",
       "      <td>0.205468</td>\n",
       "      <td>0.120054</td>\n",
       "      <td>0.109885</td>\n",
       "      <td>0.061077</td>\n",
       "      <td>0.088209</td>\n",
       "      <td>0.263467</td>\n",
       "      <td>0.295251</td>\n",
       "      <td>0.072480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75*sqrt(n)</th>\n",
       "      <td>0.302927</td>\n",
       "      <td>0.273047</td>\n",
       "      <td>0.123697</td>\n",
       "      <td>0.256664</td>\n",
       "      <td>0.145170</td>\n",
       "      <td>0.100296</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.228276</td>\n",
       "      <td>0.065239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.282099</td>\n",
       "      <td>0.338771</td>\n",
       "      <td>0.119509</td>\n",
       "      <td>0.348113</td>\n",
       "      <td>0.192096</td>\n",
       "      <td>0.096444</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.292196</td>\n",
       "      <td>0.065375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset.WINE                                Dataset.PIMA  \\\n",
       "             Classifier.GNB Classifier.MLP Classifier.RF Classifier.GNB   \n",
       "0.1                0.003405       0.006799      0.017835       0.016372   \n",
       "0.3                0.026427       0.040270      0.050753       0.030981   \n",
       "0.5                0.070718       0.141764      0.081997       0.045729   \n",
       "1.0                0.223794       0.205468      0.120054       0.109885   \n",
       "0.75*sqrt(n)       0.302927       0.273047      0.123697       0.256664   \n",
       "4.0                0.282099       0.338771      0.119509       0.348113   \n",
       "\n",
       "                                              Dataset.BC                 \\\n",
       "             Classifier.MLP Classifier.RF Classifier.GNB Classifier.MLP   \n",
       "0.1                0.014524      0.035582       0.005875       0.114598   \n",
       "0.3                0.026401      0.052811       0.030298       0.193765   \n",
       "0.5                0.031290      0.064341       0.104709       0.271186   \n",
       "1.0                0.061077      0.088209       0.263467       0.295251   \n",
       "0.75*sqrt(n)       0.145170      0.100296       0.001451       0.228276   \n",
       "4.0                0.192096      0.096444       0.001769       0.292196   \n",
       "\n",
       "                            \n",
       "             Classifier.RF  \n",
       "0.1               0.014778  \n",
       "0.3               0.038165  \n",
       "0.5               0.057385  \n",
       "1.0               0.072480  \n",
       "0.75*sqrt(n)      0.065239  \n",
       "4.0               0.065375  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5054377939270213"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.mean((results_lime - results_lemon) / results_lime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
